{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-25T06:53:46.018122Z","iopub.status.busy":"2023-03-25T06:53:46.017668Z","iopub.status.idle":"2023-03-25T06:53:46.023955Z","shell.execute_reply":"2023-03-25T06:53:46.022584Z","shell.execute_reply.started":"2023-03-25T06:53:46.018060Z"},"trusted":true},"outputs":[],"source":["# from torchtext.transforms import BERTTokenizer\n","\n","# VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\n","# tokenizer = BERTTokenizer(vocab_path=VOCAB_FILE, do_lower_case=True, never_split=[\"[CLS]\", \"[SEP]\"])\n","# tokenizer(\"Hello World, How are you!\") # single sentence input\n","# tokenizer([\"Hello World\",\"How are you!\"])"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T09:20:27.195544Z","iopub.status.busy":"2023-03-25T09:20:27.195112Z","iopub.status.idle":"2023-03-25T09:20:27.348379Z","shell.execute_reply":"2023-03-25T09:20:27.347136Z","shell.execute_reply.started":"2023-03-25T09:20:27.195511Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from transformers import BertTokenizer\n","from dataclasses import dataclass\n","from typing import Tuple, Union, List\n","\n","@dataclass\n","class BERTConfig:\n","    hidden_layers: int = 768\n","    num_heads: int = 12\n","    attention_blocks: int = 12\n","    dropout: float = 0.2\n","    vocabulary_size: int = None\n","    sequence_len: int = 32\n","\n","@dataclass\n","class MLMData:\n","    x: torch.Tensor\n","    y: torch.Tensor\n","    att_mask: torch.Tensor\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T06:53:55.837905Z","iopub.status.busy":"2023-03-25T06:53:55.837308Z","iopub.status.idle":"2023-03-25T06:53:55.893687Z","shell.execute_reply":"2023-03-25T06:53:55.892658Z","shell.execute_reply.started":"2023-03-25T06:53:55.837869Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[ 101, 7632, 2129, 2024, 2017, 1029,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"Hi how are you?\", return_tensors='pt')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T06:53:55.897231Z","iopub.status.busy":"2023-03-25T06:53:55.896154Z","iopub.status.idle":"2023-03-25T06:53:55.995944Z","shell.execute_reply":"2023-03-25T06:53:55.995115Z","shell.execute_reply.started":"2023-03-25T06:53:55.897187Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([1, 20, 64])"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import math\n","\n","class Head(nn.Module):\n","    def __init__(self, config: BERTConfig, bias: bool=False):\n","        super().__init__()\n","        self.W_q = nn.Linear(config.hidden_layers, config.hidden_layers // config.num_heads, bias=bias)\n","        self.W_k = nn.Linear(config.hidden_layers, config.hidden_layers // config.num_heads, bias=bias)\n","        self.W_v = nn.Linear(config.hidden_layers, config.hidden_layers // config.num_heads, bias=bias)\n","        self.dropout = nn.Dropout(p=config.dropout)\n","        self.softmax = nn.Softmax(dim=-1)\n","    \n","    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) -> torch.Tensor:\n","        batch_size, t, embeddings = x.shape\n","        \n","        #(batch, t, emb)\n","        q = self.W_q(x)\n","        k = self.W_k(x).transpose(1, 2)\n","        v = self.W_v(x)\n","        attention = (q @ k) * (1 / math.sqrt(embeddings))        \n","        if mask is not None:\n","            attention = attention.masked_fill(mask == 0, float('-inf'))\n","        \n","        return self.dropout(self.softmax(attention)) @ v\n","        \n","        \n","h = Head(BERTConfig)\n","test = torch.randn((1, 20, 768))\n","h.forward(test).shape"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T06:53:55.998588Z","iopub.status.busy":"2023-03-25T06:53:55.997443Z","iopub.status.idle":"2023-03-25T06:53:56.208957Z","shell.execute_reply":"2023-03-25T06:53:56.208061Z","shell.execute_reply.started":"2023-03-25T06:53:55.998522Z"},"trusted":true},"outputs":[{"data":{"text/plain":["torch.Size([32, 40, 768])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, config: BERTConfig):\n","        super().__init__()\n","        assert config.hidden_layers % config.num_heads == 0, f\"Cannot equally distribute {config.hidden_layers} to {config.num_heads} heads!\"\n","        self.attention = nn.ModuleList([Head(config) for _ in range(config.num_heads)])\n","        self.W_o = nn.Linear(config.hidden_layers, config.hidden_layers, bias=False)\n","        self.dropout = nn.Dropout(p=config.dropout)\n","    \n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \n","        result = torch.cat([head(x) for head in self.attention], dim=-1)\n","        \n","        return self.dropout(self.W_o(result))\n","        \n","mh = MultiHeadAttention(BERTConfig)\n","mh.forward(torch.randn((32, 40, 768))).shape"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T06:53:56.211300Z","iopub.status.busy":"2023-03-25T06:53:56.210606Z","iopub.status.idle":"2023-03-25T06:53:56.221788Z","shell.execute_reply":"2023-03-25T06:53:56.220795Z","shell.execute_reply.started":"2023-03-25T06:53:56.211262Z"},"trusted":true},"outputs":[],"source":["class Block(nn.Module):\n","    \n","    def __init__(self, config: BERTConfig):\n","        super().__init__()\n","        self.norm_1 = nn.LayerNorm(config.hidden_layers)\n","        self.norm_2 = nn.LayerNorm(config.hidden_layers)\n","        self.fully_connected = nn.Sequential(\n","            nn.Linear(config.hidden_layers, config.hidden_layers*4),\n","            nn.GELU(),\n","            nn.Linear(config.hidden_layers*4, config.hidden_layers),\n","            nn.Dropout(p=config.dropout)\n","        )\n","        self.multi_head_att = MultiHeadAttention(config)\n","    \n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = x + self.multi_head_att(self.norm_1(x))\n","        x = x + self.fully_connected(self.norm_2(x))\n","        return x\n","\n","class BERTEmbeddings(nn.Module):\n","    \n","    def __init__(self, config: BERTConfig):\n","        super().__init__()\n","        self.embeddings = nn.Embedding(config.vocabulary_size, config.hidden_layers)\n","        self.positional_embeddings = nn.Embedding(config.sequence_len, config.hidden_layers)\n","        self.segment_embeddings = nn.Embedding(2, config.hidden_layers)\n","        self.emb_norm = nn.LayerNorm(config.hidden_layers)        \n","    \n","    def forward(self, tokens: torch.Tensor, token_type_ids: torch.Tensor) -> torch.Tensor:\n","        x = self.embeddings(tokens) + self.positional_embeddings(tokens) + self.segment_embeddings(input_ids)\n","        return self.emb_norm(x)\n","    \n","class BERT(nn.Module):\n","    \n","    def __init__(self, config: BERTConfig):\n","        super().__init__()\n","        self.blocks = nn.ModuleList([Block(config) for _ in range(config.attention_blocks)])\n","        self.embeddings = BERTEmbeddings(config)\n","    \n","    def forward(self,\n","                x: torch.Tensor,\n","                attention_mask: torch.Tensor=None,\n","                input_token_ids: torch.Tensor=None,\n","                targets: torch.Tensor=None) -> Tuple[torch.Tensor, torch.Tensor]:\n","        ..."]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T08:55:23.587885Z","iopub.status.busy":"2023-03-25T08:55:23.586562Z","iopub.status.idle":"2023-03-25T08:55:23.609967Z","shell.execute_reply":"2023-03-25T08:55:23.609039Z","shell.execute_reply.started":"2023-03-25T08:55:23.587835Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["MLMData(x=tensor([ 101,  103, 2024, 2017, 1029,  102, 2242,  102,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0]), y=tensor([ 101, 2129, 2024, 2017, 1029,  102, 2242,  102,   -1,   -1,   -1,   -1,\n","          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n","          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n","          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n","          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n","          -1,   -1,   -1,   -1]), att_mask=tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"]},{"data":{"text/plain":["'[CLS] [MASK] are you? [SEP] something [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["class MLM(Dataset):\n","    \n","    def __init__(self, tokenizer, data: list, mask_idx: int, sequence_length: int=64):\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        vocab_idxs = torch.ones((tokenizer.vocab_size,))\n","        vocab_idxs[0:1996] = 0\n","        v_len = len(vocab_idxs[1996:])\n","        #equal probability distribution over tokens that are litterals.\n","        self.sample_from_vocab = (vocab_idxs / v_len)\n","        #0.8 - probability of selecting [MASK] token,  0.1 respectively probability of random token from vocabulary or not replacing a word at all\n","        self.mask_idx = torch.tensor([0.8, 0.1, 0.1])\n","        self.mask_mapping = {0: lambda: 103, 1: lambda: torch.multinomial(self.sample_from_vocab, num_samples=1).item()}\n","        self.data = data\n","        self.seq_len = sequence_length\n","    \n","    def __len__(self) -> int:\n","        return len(self.data)\n","    \n","    def encode(self, data: Union[str, List[str]]) -> MLMData:\n","        \n","        tokenizer_data = self.tokenizer(data, return_tensors='pt', \n","                                max_length=self.seq_len,\n","                                truncation=True,\n","                                padding='max_length')\n","        #encoded tokens\n","        tokens = tokenizer_data['input_ids'].reshape(-1)\n","        #replace padding tokens with index -1 so it willl be skipped in loss_fn\n","        y = tokens.masked_fill(tokens == 0, -1)\n","        att_mask = tokenizer_data['attention_mask'].reshape(-1)\n","        #filter [CLS] [SEP] tokens to not replace them with [MASK]\n","        special_tokens_filter = tokens.eq(102) | tokens.eq(101)\n","        #equal probability distribution over tokens excluding [SEP], [CLS]\n","        samples = att_mask.masked_fill(special_tokens_filter, 0) * 0.15\n","        \n","        #draw one index that will decide which masking will be used\n","        mask_type = torch.multinomial(self.mask_idx, num_samples=1).item()\n","        selected = torch.bernoulli(samples)\n","        #fill drawed indexes with masking method\n","        x = tokens.masked_fill(selected == 1, self.mask_mapping[mask_type]()) if mask_type != 2 else tokens\n","        return MLMData(x, y, att_mask)\n","    \n","    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n","        ...\n","        \n","mlm = MLM(tokenizer, [], 103)\n","obj = mlm.encode(\"how are you? [SEP] something\")\n","print(obj)\n","tokenizer.decode(obj.x)"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T09:02:27.062381Z","iopub.status.busy":"2023-03-25T09:02:27.061963Z","iopub.status.idle":"2023-03-25T09:02:27.073012Z","shell.execute_reply":"2023-03-25T09:02:27.071707Z","shell.execute_reply.started":"2023-03-25T09:02:27.062345Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[ 101, 2129, 2024, 2017, 1029,  102, 2242,  102,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])}"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer(\"how are you?\",\"something\", truncation=True, max_length=10, padding='max_length', return_tensors='pt')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T06:53:56.290919Z","iopub.status.busy":"2023-03-25T06:53:56.289775Z","iopub.status.idle":"2023-03-25T06:53:56.301085Z","shell.execute_reply":"2023-03-25T06:53:56.299474Z","shell.execute_reply.started":"2023-03-25T06:53:56.290880Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([0])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["torch.multinomial(torch.tensor([0.8, 0.1, 0.1]), 1)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T06:53:56.303942Z","iopub.status.busy":"2023-03-25T06:53:56.303090Z","iopub.status.idle":"2023-03-25T06:53:56.312398Z","shell.execute_reply":"2023-03-25T06:53:56.310829Z","shell.execute_reply.started":"2023-03-25T06:53:56.303877Z"},"trusted":true},"outputs":[{"data":{"text/plain":["30522"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.vocab_size"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T07:52:56.388269Z","iopub.status.busy":"2023-03-25T07:52:56.387841Z","iopub.status.idle":"2023-03-25T07:52:56.398290Z","shell.execute_reply":"2023-03-25T07:52:56.396867Z","shell.execute_reply.started":"2023-03-25T07:52:56.388232Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([1, 1, 1, 1], dtype=torch.int8)"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["torch.ones_like(torch.tensor([True, False, False, True]) == True, dtype=torch.int8)"]},{"cell_type":"code","execution_count":107,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T10:41:34.383193Z","iopub.status.busy":"2023-03-25T10:41:34.382770Z","iopub.status.idle":"2023-03-25T10:41:34.390798Z","shell.execute_reply":"2023-03-25T10:41:34.389503Z","shell.execute_reply.started":"2023-03-25T10:41:34.383153Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.9266, 0.0050, 0.0127, 0.0557],\n","        [0.2556, 0.2382, 0.2435, 0.2628],\n","        [0.3875, 0.1447, 0.1787, 0.2891],\n","        [0.4397, 0.0404, 0.0749, 0.4450]])\n"]}],"source":["t = torch.tensor([[0.1, 2.3], [0.06, 0.03], [0.23, 0.43], [1.2, 1.03]])\n","print(F.softmax((t @ t.T), dim=-1))"]},{"cell_type":"code","execution_count":165,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T11:19:42.745360Z","iopub.status.busy":"2023-03-25T11:19:42.744287Z","iopub.status.idle":"2023-03-25T11:19:42.757313Z","shell.execute_reply":"2023-03-25T11:19:42.756145Z","shell.execute_reply.started":"2023-03-25T11:19:42.745318Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[5.3000e+00, 7.5000e-02, 1.0000e-10, 2.4890e+00],\n","        [7.5000e-02, 4.5000e-03, 1.0000e-10, 1.0290e-01],\n","        [1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10],\n","        [2.4890e+00, 1.0290e-01, 1.0000e-10, 2.5009e+00]])"]},"execution_count":165,"metadata":{},"output_type":"execute_result"}],"source":["t = torch.tensor([[0.1, 2.3], [0.06, 0.03], [0.23, 0.43], [1.2, 1.03]])\n","v = torch.tensor([[0.1, 2.3], [0.06, 0.03], [0.23, 0.43], [1.2, 1.03]])\n","mak = torch.tensor([1, 1, 0, 1]).expand(4, 4)\n","mask = mak.transpose(-2, -1) @ mak\n","# mask\n","t = t @ t.T\n","t = t.masked_fill(mask == 0, 1e-10)\n","t\n","# F.softmax(t, dim=-1) @ v\n","\n","# (mak.T @ mak).unsqueeze(0).unsqueeze(0).shape\n","# mak.eq(0)\n","# mm\n","\n","# F.softmax(test @ test.T, dim=-1)"]},{"cell_type":"code","execution_count":168,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T11:21:21.674496Z","iopub.status.busy":"2023-03-25T11:21:21.673988Z","iopub.status.idle":"2023-03-25T11:21:21.683925Z","shell.execute_reply":"2023-03-25T11:21:21.682416Z","shell.execute_reply.started":"2023-03-25T11:21:21.674457Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([0.2500, 0.2500, 0.2500, 0.2500])"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["F.softmax(torch.tensor([1.0000e-10, 1.0000e-10, 1.0000e-10, 1.0000e-10]), dim=-1)"]},{"cell_type":"code","execution_count":184,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T11:38:48.644733Z","iopub.status.busy":"2023-03-25T11:38:48.644357Z","iopub.status.idle":"2023-03-25T11:38:48.654498Z","shell.execute_reply":"2023-03-25T11:38:48.652810Z","shell.execute_reply.started":"2023-03-25T11:38:48.644699Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[0.2500, 0.2500, 0.2500, 0.2500],\n","        [0.2500, 0.2500, 0.2500, 0.2500],\n","        [0.2500, 0.2500, 0.2500, 0.2500],\n","        [0.2500, 0.2500, 0.2500, 0.2500]])"]},"execution_count":184,"metadata":{},"output_type":"execute_result"}],"source":["F.softmax(torch.tensor([0, 0, -10000.0, 0]).unsqueeze(1) * (t @t.T),  dim=1)\n","# torch.tensor([0, 0, -10000.0, 0]).unsqueeze(0).shape"]},{"cell_type":"code","execution_count":160,"metadata":{"execution":{"iopub.execute_input":"2023-03-25T11:17:25.021913Z","iopub.status.busy":"2023-03-25T11:17:25.020506Z","iopub.status.idle":"2023-03-25T11:17:25.032657Z","shell.execute_reply":"2023-03-25T11:17:25.030954Z","shell.execute_reply.started":"2023-03-25T11:17:25.021852Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n","        [0.6014, 0.3986, 0.0000, 0.0000],\n","        [0.6092, 0.3370, 0.0539, 0.0000],\n","        [0.6851, 0.1298, 0.0614, 0.1238]])"]},"execution_count":160,"metadata":{},"output_type":"execute_result"}],"source":["F.softmax(torch.randn((4,4)).masked_fill(torch.tril(torch.ones(4,4)) == 0, float('-inf')), dim=-1)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
